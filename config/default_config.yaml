# MCP Client Default Configuration

# LLM provider configuration
llm:
  provider: openai        # Options: ollama, deepseek, openai, openrouter
  model: GLM-4-Flash      # Default model for the selected provider
  api_key: "fd69c68ffab3452da1e00bbf6bd4c915.axvFwrXXiDDnJXKx"       # API key for providers that require it (OpenAI, Deepseek, OpenRouter)
  api_base: "https://open.bigmodel.cn/api/paas/v4/"      # Optional API base URL override
  
  # Provider-specific configurations
  ollama:
    host: "http://localhost:11434"
    models:
      - llama3
      - mistral
      - gemma
  
  deepseek:
    models:
      - deepseek-chat
      - deepseek-coder
  
  openai:
    models:
      - gpt-4
      - gpt-3.5-turbo
  
  openrouter:
    models:
      - anthropic/claude-3-opus
      - google/gemini-pro
      - meta-llama/llama-3-70b-instruct

# MCP server configurations
mcp_servers:
  k8s_server:  # Default K8s server
    url: "http://192.168.182.128:8000/sse"  # SSE transport URL
    transport: "sse"              # Transport type: "sse" or "stdio"
    enabled: true                 # Whether this server is enabled
    default: true                 # Whether this is the default server
    # stdio_command: "python -m mcp_k8s_server"  # Command to start the server in stdio mode

# Console configuration
console:
  history_file: "~/.mcp_client_history"  # Command history file
  log_level: "info"                      # Logging level: debug, info, warning, error
  max_history: 1000                      # Maximum number of history entries to keep
  prompt: "mcp> "                        # Console prompt
